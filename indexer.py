# indexer.py
import sqlite3
import requests
from bs4 import BeautifulSoup
import openai
import json
import time
import random
import logging

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    from config import OPENAI_API_KEY
except ImportError:
    logger.error("Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ config.py")
    exit()

# ØªÙ‡ÙŠØ¦Ø© AI
openai.api_key = OPENAI_API_KEY
DB_NAME = "books_index.db"

# ===================================================
# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ===================================================
def setup_db():
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS books (
            id INTEGER PRIMARY KEY,
            title TEXT UNIQUE,
            author TEXT,
            source_url TEXT,
            download_link TEXT,
            summary TEXT
        )
    """)
    conn.commit()
    return conn

# ===================================================
# 2. Ø¯Ø§Ù„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ AI Ù„Ù„ØªÙ†Ø¸ÙŠÙ
# ===================================================
def clean_data_with_ai(raw_title, raw_summary):
    """ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù€ AI Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù„ÙƒØªØ§Ø¨."""
    prompt = f"""
    Ù‚Ù… Ø¨ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:
    1. Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙˆØ§Ù„Ù…Ø¤Ù„Ù Ù…Ù† '{raw_title}'.
    2. Ù„Ø®Øµ '{raw_summary}' ÙÙŠ Ø¬Ù…Ù„ØªÙŠÙ† ÙÙ‚Ø·.
    
    Ø±Ø¯ Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø·: {{'title': '...', 'author': '...', 'summary': '...'}}
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a data cleaning and summarization expert. Respond ONLY with the requested JSON object."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            max_tokens=250
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AI: {e}")
        return None

# ===================================================
# 3. Ø¯Ø§Ù„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ù…Ø¹ÙŠÙ† (Ù…Ø«Ø§Ù„: Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù†ÙˆØ±)
# *Ù…Ù„Ø§Ø­Ø¸Ø©: ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ®ØµÙŠØµ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹*
# ===================================================
def scrape_noorbook(conn):
    base_url = "https://www.noor-book.com/ar/books"
    logger.info(f"Ø¨Ø¯Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†: {base_url}")
    
    # Ø³Ù†ÙÙ‡Ø±Ø³ Ø£ÙˆÙ„ 5 ØµÙØ­Ø§Øª ÙƒÙ…Ø«Ø§Ù„ (Ù„Ø£Ù† Ø§Ù„ÙÙ‡Ø±Ø³Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù…Ø¹ AI ØªØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ ÙˆØªÙƒÙ„ÙØ©)
    for page in range(1, 6): 
        try:
            response = requests.get(f"{base_url}?page={page}", timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„ ÙƒØ§Ø±Øª ÙƒØªØ§Ø¨
            book_cards = soup.find_all('div', class_='book-card') 

            for card in book_cards:
                try:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· ØµÙØ­Ø© Ø§Ù„ÙƒØªØ§Ø¨
                    book_link = card.find('a', class_='book-cover')['href']
                    full_link = f"https://www.noor-book.com{book_link}"
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø£ÙˆÙ„ÙŠ (Ù‚Ø¯ ÙŠÙƒÙˆÙ† ØºÙŠØ± Ù†Ø¸ÙŠÙ)
                    raw_title = card.find('h4', class_='book-title').text.strip()
                    
                    # **[Ù‡Ù†Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø±Ø© ÙƒÙ„ ØµÙØ­Ø© Ø¬ÙØ²Ø¦ÙŠØ§Ù‹ Ù„Ø¬Ù„Ø¨ Ø±Ø§Ø¨Ø· Ø§Ù„ØªÙ†Ø²ÙŠÙ„ ÙˆØ§Ù„Ù…Ù„Ø®Øµ]**
                    
                    # *Ù„ØºØ±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ØŒ Ø³Ù†ÙØªØ±Ø¶ Ø£Ù†Ù†Ø§ Ø­ØµÙ„Ù†Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª*
                    raw_summary = "Ù…Ù„Ø®Øµ Ù…Ø¤Ù‚Øª Ù„ØºØ±Ø¶ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±."
                    download_link = "http://example.com/download/book.pdf" 
                    
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… AI Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    ai_data = clean_data_with_ai(raw_title, raw_summary)
                    
                    if ai_data:
                        cursor = conn.cursor()
                        cursor.execute("""
                            INSERT OR IGNORE INTO books 
                            (title, author, source_url, download_link, summary)
                            VALUES (?, ?, ?, ?, ?)
                        """, (
                            ai_data['title'],
                            ai_data['author'],
                            full_link,
                            download_link,
                            ai_data['summary']
                        ))
                        conn.commit()
                        logger.info(f"âœ… ØªÙ… ÙÙ‡Ø±Ø³Ø©: {ai_data['title']}")
                    
                    # Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ù†Ø¹ Ø§Ù„Ø­Ø¸Ø±
                    time.sleep(random.uniform(2, 5)) 

                except Exception as e:
                    logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒØ§Ø±Øª ÙƒØªØ§Ø¨: {e}")
                    continue
            
            logger.info(f"Ø§Ù†ØªÙ‡Øª Ø§Ù„ØµÙØ­Ø© Ø±Ù‚Ù…: {page}")
        
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© {page}: {e}")
            break

# ===================================================
# 4. Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ===================================================
def main_indexer():
    conn = setup_db()
    
    # ØªÙ†ÙÙŠØ° Ø§Ù„ÙÙ‡Ø±Ø³Ø© Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹
    scrape_noorbook(conn) 
    # ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ø¯ÙˆØ§Ù„ Ù„Ù€ "scrape_kotobati" Ùˆ "scrape_alkutub" Ù‡Ù†Ø§.

    conn.close()
    logger.info("ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙÙ‡Ø±Ø³Ø© Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±!")

if __name__ == "__main__":
    main_indexer()
